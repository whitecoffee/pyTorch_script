{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([544, 30])\n",
      "torch.Size([152, 30])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ===================Train Data======================= #\n",
    "#\n",
    "#  输入30个采样点，预测30个采样点。由于每个\n",
    "#  采样点都包含3个坐标(x,y,z)，所以需要输入\n",
    "#  维度为90，输出维度为90。\n",
    "#\n",
    "#  每段轨迹长度3秒，共90个采样点。最后30个\n",
    "#  采样点不需要作为输入，前30个采样点不需要\n",
    "#  作为预测的真实值输出。前60个采样点作为输\n",
    "#  入\n",
    "#  \n",
    "#  N = landmarks.shape[0] - landmarks.shape[0]/9\n",
    "#\n",
    "#  range取值是轨迹的条数\n",
    "# =====================================================\n",
    "\n",
    "# FC-net architecture\n",
    "D_in, H_in, H_out, D_out = 30, 15, 15, 30 \n",
    "Z_dim, Learning_rate = 3, 1e-3\n",
    "#D_in, H_in, H_out, D_out = 20, 15, 15, 10\n",
    "\n",
    "landmarks_frame = pd.read_csv('Try.csv')\n",
    "landmarks = landmarks_frame.as_matrix().astype('float')\n",
    "\n",
    "# Normalization\n",
    "#maxdata = np.array([1.5, 0.4, 1.2])\n",
    "#mindata = np.array([0.8, -0.5, 0.6])\n",
    "maxdata = np.max(landmarks, axis=0)\n",
    "mindata = np.min(landmarks, axis=0)\n",
    "landmarks = (landmarks - mindata)/(maxdata - mindata)\n",
    "landmarks = landmarks.astype('float').reshape(-1, D_in)\n",
    "\n",
    "\n",
    "N = landmarks.shape[0] * 8 / 9\n",
    "N = int(N)\n",
    "count = landmarks.shape[0] / 9\n",
    "count = int(count)\n",
    "#print('Landmarks shape: {}'.format(landmarks.shape))\n",
    "x = torch.zeros(N, D_in)\n",
    "y = torch.zeros(N, D_out)\n",
    "\n",
    "for i in range(count):\n",
    "    x[i*8:i*8+8] = Variable(torch.from_numpy(landmarks[i*9:i*9+8]), requires_grad = False)\n",
    "    y[i*8:i*8+8] = Variable(torch.from_numpy(landmarks[i*9+1:i*9+9, :D_out]), requires_grad = False)\n",
    "print(x.shape)\n",
    "\n",
    "torch.manual_seed(1)    # reproducible\n",
    "torch_dataset = Data.TensorDataset(x, y)\n",
    "# 把 dataset 放入 DataLoader\n",
    "loader = Data.DataLoader(\n",
    "    dataset=torch_dataset,      # torch TensorDataset format\n",
    "    batch_size=8,               # mini batch size\n",
    "    shuffle=True,               # 要不要打乱数据 (打乱比较好)\n",
    "    num_workers=2,              # 多线程来读数据\n",
    ")\n",
    "\n",
    "\n",
    "# ===================Validation Data======================= #\n",
    "landmarks_frame_val = pd.read_csv('Val.csv')\n",
    "landmarks_val = landmarks_frame_val.as_matrix().astype('float')\n",
    "\n",
    "landmarks_val = (landmarks_val - mindata)/(maxdata - mindata)\n",
    "landmarks_val = landmarks_val.astype('float').reshape(-1, D_in)\n",
    "\n",
    "#print('Landmarks shape: {}'.format(landmarks_val.shape))\n",
    "\n",
    "N_val = landmarks_val.shape[0] * 8 / 9\n",
    "N_val = int(N_val)\n",
    "count_val = landmarks_val.shape[0] / 9\n",
    "count_val = int(count_val)\n",
    "\n",
    "x_val = torch.zeros(N_val, D_in)\n",
    "y_val = torch.zeros(N_val, D_out)\n",
    "\n",
    "for i in range(count_val):\n",
    "    x_val[i*8:i*8+8] = Variable(torch.from_numpy(landmarks_val[i*9:i*9+8]), requires_grad = False)\n",
    "    y_val[i*8:i*8+8] = Variable(torch.from_numpy(landmarks_val[i*9+1:i*9+9, :D_out]), requires_grad = False)\n",
    "\n",
    "print(x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" VAE模型\n",
    "\n",
    "自定义初始化\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / np.sqrt(in_dim / 2.)\n",
    "    return Variable(torch.randn(*size) * xavier_stddev, requires_grad=True)\n",
    "\n",
    "或调用官方方法\n",
    ">>> w = torch.empty(3, 5)\n",
    ">>> torch.nn.init.xavier_normal_(w)\n",
    "\"\"\"\n",
    "\n",
    "class vae(torch.nn.Module):\n",
    "    def __init__(self, z_dim, x_dim, y_dim, h_dim):\n",
    "        super(vae, self).__init__()\n",
    "        self.Wxh = Variable(torch.nn.init.xavier_normal_(torch.empty(h_dim, x_dim)), requires_grad=True)\n",
    "        self.bxh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "     \n",
    "        self.Whz_mu = Variable(torch.nn.init.xavier_normal_(torch.empty(z_dim, h_dim)), requires_grad=True)  # 需要更改\n",
    "        self.bhz_mu = Variable(torch.zeros(z_dim), requires_grad=True)\n",
    "\n",
    "        self.Whz_var = Variable(torch.nn.init.xavier_normal_(torch.empty(z_dim, h_dim)), requires_grad=True)  # 需要更改\n",
    "        self.bhz_var = Variable(torch.zeros(z_dim), requires_grad=True)\n",
    "\n",
    "        #self.Wzh_tem = Variable(torch.nn.init.xavier_normal_(torch.empty(h_dim, z_dim)), requires_grad=True)\n",
    "        #self.bzh_tem = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "        \n",
    "        self.Wzx = Variable(torch.nn.init.xavier_normal_(torch.empty(y_dim, z_dim)), requires_grad=True)  # 需要更改\n",
    "        self.bzx = Variable(torch.zeros(y_dim), requires_grad=True)\n",
    "        \n",
    "        self.params = [self.Whz_mu, self.bhz_mu, self.Whz_var, self.bhz_var, self.Wzx, self.bzx]\n",
    "        self.params.append(self.Wxh)\n",
    "        self.params.append(self.bxh)\n",
    "        #self.params.append(self.Wzh_tem)\n",
    "        #self.params.append(self.bzh_tem)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # See the autograd section for explanation of what happens here.\n",
    "        self.z_mu, self.z_var = self.Q(x)\n",
    "        z = self.sample_z(self.z_mu, self.z_var)\n",
    "        y_pred = self.P(z)\n",
    "        y_pred = y_pred + x\n",
    "        return y_pred\n",
    "    \n",
    "    # =============================== Q(z|X) ======================================\n",
    "    def Q(self, x):\n",
    "        h = F.relu(F.linear(x, self.Wxh, self.bxh))\n",
    "        z_mu = F.linear(h, self.Whz_mu, self.bhz_mu)\n",
    "        z_var = F.linear(h, self.Whz_var, self.bhz_var)\n",
    "        return z_mu, z_var\n",
    "\n",
    "    def sample_z(self, mu, log_var):\n",
    "        eps = Variable(torch.randn(mu.shape[0], mu.shape[1]))\n",
    "        return mu + torch.exp(log_var / 2) * eps\n",
    "\n",
    "    # =============================== P(X|z) ======================================\n",
    "    def P(self, z):\n",
    "        #h = F.linear(z, self.Wzh_tem, self.bzh_tem)\n",
    "        X = F.linear(z, self.Wzx, self.bzx)\n",
    "        return X\n",
    "    \n",
    "# =============================== TRAINING ====================================  \n",
    "model = vae(z_dim = Z_dim, x_dim = D_in, y_dim = D_out, h_dim = H_in)   \n",
    "optimizer = torch.optim.Adam(model.params, lr=Learning_rate)\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-0; Loss: 3.213e+03\n",
      "Iter-1000; Loss: 82.02\n",
      "Iter-2000; Loss: 54.14\n",
      "Iter-3000; Loss: 33.33\n",
      "Iter-4000; Loss: 19.03\n",
      "Iter-5000; Loss: 16.65\n",
      "Iter-6000; Loss: 15.04\n",
      "Iter-7000; Loss: 13.5\n",
      "Iter-8000; Loss: 13.22\n",
      "Iter-9000; Loss: 12.91\n",
      "Iter-10000; Loss: 12.5\n",
      "Iter-11000; Loss: 12.28\n",
      "Iter-12000; Loss: 12.04\n",
      "Iter-13000; Loss: 12.17\n",
      "Iter-14000; Loss: 12.26\n",
      "Iter-15000; Loss: 11.87\n",
      "Iter-16000; Loss: 11.69\n",
      "Iter-17000; Loss: 11.66\n"
     ]
    }
   ],
   "source": [
    "# Whole batch\n",
    "\n",
    "start = time.time()\n",
    "for t in range(20000):\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    recon_loss = F.mse_loss(y_pred, y, size_average=False)\n",
    "    kl_loss = torch.mean(0.5 * torch.sum(torch.exp(model.z_var) + model.z_mu**2 - 1. - model.z_var, 1))\n",
    "    loss = recon_loss + kl_loss\n",
    "    \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #scheduler.step(loss)\n",
    "    \n",
    "    # Print and plot every now and then\n",
    "    if t % 1000 == 0:\n",
    "        print('Iter-{}; Loss: {:.4}'.format(t, recon_loss.data[0]))\n",
    "end = time.time()\n",
    "print('VAE net cal_time is {}'.format(end-start))\n",
    "y_pred_val = model(x_val)\n",
    "recon_loss_val = F.mse_loss(y_pred_val, y_val, size_average=False)\n",
    "print('VAE val_loss is {}'.format(recon_loss_val.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
