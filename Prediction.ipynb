{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ===================Train Data======================= #\n",
    "#\n",
    "#  输入30个采样点，预测30个采样点。由于每个\n",
    "#  采样点都包含3个坐标(x,y,z)，所以需要输入\n",
    "#  维度为90，输出维度为90。\n",
    "#\n",
    "#  每段轨迹长度3秒，共90个采样点。最后30个\n",
    "#  采样点不需要作为输入，前30个采样点不需要\n",
    "#  作为预测的真实值输出。前60个采样点作为输\n",
    "#  入\n",
    "#  \n",
    "#  N = landmarks.shape[0] - landmarks.shape[0]/9\n",
    "#\n",
    "#  range取值是轨迹的条数\n",
    "# =====================================================\n",
    "\n",
    "# FC-net architecture\n",
    "D_in, H_in, H_out, D_out = 30, 10, 10, 15 \n",
    "Z_dim, Learning_rate = 3, 1e-3\n",
    "#D_in, H_in, H_out, D_out = 20, 15, 15, 10\n",
    "\n",
    "landmarks_frame = pd.read_csv('Try.csv')\n",
    "landmarks = landmarks_frame.as_matrix().astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55080, 3)\n"
     ]
    }
   ],
   "source": [
    "plus = np.random.randint(low=1, high=2, size=landmarks.shape)\n",
    "plus = plus/10000\n",
    "plus = landmarks + plus\n",
    "tempo = np.append(landmarks, plus, axis=0)\n",
    "\n",
    "sub = np.random.randint(low=1, high=4, size=landmarks.shape)\n",
    "sub = sub/10000\n",
    "sub = landmarks - sub\n",
    "landmarks = np.append(tempo, sub, axis=0)\n",
    "print(landmarks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([376, 30])\n",
      "torch.Size([120, 30])\n"
     ]
    }
   ],
   "source": [
    "# Normalization\n",
    "#maxdata = np.array([1.5, 0.4, 1.2])\n",
    "#mindata = np.array([0.8, -0.5, 0.6])\n",
    "maxdata = np.max(landmarks, axis=0)\n",
    "mindata = np.min(landmarks, axis=0)\n",
    "landmarks = (landmarks - mindata)/(maxdata - mindata)\n",
    "landmarks = landmarks.astype('float').reshape(-1, D_in)\n",
    "\n",
    "\n",
    "N = landmarks.shape[0] * 8 / 9\n",
    "N = int(N)\n",
    "count = landmarks.shape[0] / 9\n",
    "count = int(count)\n",
    "#print('Landmarks shape: {}'.format(landmarks.shape))\n",
    "x = torch.zeros(N, D_in)\n",
    "y = torch.zeros(N, D_out)\n",
    "\n",
    "for i in range(count):\n",
    "    x[i*8:i*8+8] = Variable(torch.from_numpy(landmarks[i*9:i*9+8]), requires_grad = False)\n",
    "    y[i*8:i*8+8] = Variable(torch.from_numpy(landmarks[i*9+1:i*9+9, :D_out]), requires_grad = False)\n",
    "print(x.shape)\n",
    "\n",
    "torch.manual_seed(1)    # reproducible\n",
    "torch_dataset = Data.TensorDataset(x, y)\n",
    "# 把 dataset 放入 DataLoader\n",
    "loader = Data.DataLoader(\n",
    "    dataset=torch_dataset,      # torch TensorDataset format\n",
    "    batch_size=8,               # mini batch size\n",
    "    shuffle=True,               # 要不要打乱数据 (打乱比较好)\n",
    "    num_workers=2,              # 多线程来读数据\n",
    ")\n",
    "\n",
    "\n",
    "# ===================Validation Data======================= #\n",
    "landmarks_frame_val = pd.read_csv('Val.csv')\n",
    "landmarks_val = landmarks_frame_val.as_matrix().astype('float')\n",
    "\n",
    "landmarks_val = (landmarks_val - mindata)/(maxdata - mindata)\n",
    "landmarks_val = landmarks_val.astype('float').reshape(-1, D_in)\n",
    "\n",
    "#print('Landmarks shape: {}'.format(landmarks_val.shape))\n",
    "\n",
    "N_val = landmarks_val.shape[0] * 8 / 9\n",
    "N_val = int(N_val)\n",
    "count_val = landmarks_val.shape[0] / 9\n",
    "count_val = int(count_val)\n",
    "\n",
    "x_val = torch.zeros(N_val, D_in)\n",
    "y_val = torch.zeros(N_val, D_out)\n",
    "\n",
    "for i in range(count_val):\n",
    "    x_val[i*8:i*8+8] = Variable(torch.from_numpy(landmarks_val[i*9:i*9+8]), requires_grad = False)\n",
    "    y_val[i*8:i*8+8] = Variable(torch.from_numpy(landmarks_val[i*9+1:i*9+9, :D_out]), requires_grad = False)\n",
    "\n",
    "print(x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-0; Loss: 2.021e+03\n",
      "Iter-100; Loss: 220.4\n",
      "Iter-200; Loss: 131.9\n",
      "Iter-300; Loss: 79.15\n",
      "Iter-400; Loss: 51.01\n",
      "Iter-500; Loss: 36.26\n",
      "Iter-600; Loss: 27.76\n",
      "Iter-700; Loss: 22.47\n",
      "Iter-800; Loss: 19.07\n",
      "Iter-900; Loss: 16.74\n",
      "Iter-1000; Loss: 15.02\n",
      "Iter-1100; Loss: 13.62\n",
      "Iter-1200; Loss: 12.42\n",
      "Iter-1300; Loss: 11.39\n",
      "Iter-1400; Loss: 10.49\n",
      "Iter-1500; Loss: 9.705\n",
      "Iter-1600; Loss: 9.019\n",
      "Iter-1700; Loss: 8.417\n",
      "Iter-1800; Loss: 7.885\n",
      "Iter-1900; Loss: 7.415\n",
      "FC net cal_time is 0.8267889022827148\n",
      "FC net val_loss is 2.627199649810791\n"
     ]
    }
   ],
   "source": [
    "# 全连接\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H_in),\n",
    "    #torch.nn.ReLU(),\n",
    "    #torch.nn.Linear(H_in, H_out),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H_out, D_out),\n",
    "    \n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=Learning_rate)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=Learning_rate, momentum=0.9)\n",
    "#scheduler = ReduceLROnPlateau(optimizer, 'min', 1e-5, 2, threshold=1e-7 )\n",
    "\n",
    "# 载入训练号的参数\n",
    "#model.load_state_dict(torch.load('params_prone.pkl'))\n",
    "\n",
    "start = time.time()\n",
    "for t in range(2000):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #scheduler.step(loss)\n",
    "    \n",
    "    # Print and plot every now and then\n",
    "    if t % 100 == 0:\n",
    "        print('Iter-{}; Loss: {:.4}'.format(t, loss.data[0]))\n",
    "end = time.time()\n",
    "print('FC net cal_time is {}'.format(end-start))\n",
    "\n",
    "y_pred_val = model(x_val)    \n",
    "loss_val = loss_fn(y_pred_val, y_val)\n",
    "print('FC net val_loss is {}'.format(loss_val.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-0; Loss: 6.997\n",
      "Iter-1000; Loss: 4.36\n",
      "Iter-2000; Loss: 2.811\n",
      "Iter-3000; Loss: 2.036\n",
      "Iter-4000; Loss: 1.57\n",
      "Iter-5000; Loss: 1.303\n",
      "Iter-6000; Loss: 1.178\n",
      "Iter-7000; Loss: 1.102\n",
      "Iter-8000; Loss: 1.05\n",
      "Iter-9000; Loss: 1.009\n",
      "Iter-10000; Loss: 0.967\n",
      "Iter-11000; Loss: 0.871\n",
      "Iter-12000; Loss: 0.8308\n",
      "Iter-13000; Loss: 0.8104\n",
      "Iter-14000; Loss: 0.7931\n",
      "Iter-15000; Loss: 0.7803\n",
      "Iter-16000; Loss: 0.7797\n",
      "Iter-17000; Loss: 0.7602\n",
      "FC net val_loss is 0.6690676212310791\n"
     ]
    }
   ],
   "source": [
    "for t in range(18000):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #scheduler.step(loss)\n",
    "    \n",
    "    # Print and plot every now and then\n",
    "    if t % 1000 == 0:\n",
    "        print('Iter-{}; Loss: {:.4}'.format(t, loss.data[0]))\n",
    "\n",
    "y_pred_val = model(x_val)    \n",
    "loss_val = loss_fn(y_pred_val, y_val)\n",
    "print('FC net val_loss is {}'.format(loss_val.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-0; Loss: 1.398e+03\n",
      "Iter-100; Loss: 266.1\n",
      "Iter-200; Loss: 138.2\n",
      "Iter-300; Loss: 53.91\n",
      "Iter-400; Loss: 34.24\n",
      "Iter-500; Loss: 24.07\n",
      "Iter-600; Loss: 17.78\n",
      "Iter-700; Loss: 14.16\n",
      "Iter-800; Loss: 11.83\n",
      "Iter-900; Loss: 10.16\n",
      "Iter-1000; Loss: 8.851\n",
      "Iter-1100; Loss: 7.774\n",
      "Iter-1200; Loss: 6.869\n",
      "Iter-1300; Loss: 6.106\n",
      "Iter-1400; Loss: 5.461\n",
      "Iter-1500; Loss: 4.914\n",
      "Iter-1600; Loss: 4.448\n",
      "Iter-1700; Loss: 4.046\n",
      "Iter-1800; Loss: 3.697\n",
      "Iter-1900; Loss: 3.392\n",
      "LSTM cal_time is 185.18163895606995\n",
      "LSTM val_loss is 1.6534544229507446\n"
     ]
    }
   ],
   "source": [
    "\"\"\" LSTM\n",
    "\n",
    "将整个输入看作一个序列，序列长度即维度0的大小\n",
    "(seq_len, batch_size, dim)\n",
    "\"\"\"\n",
    "class lstm(torch.nn.Module):\n",
    "    def __init__(self, d_in, d_out, num_layer, h_in):\n",
    "        super(lstm, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(h_in, h_in, num_layer)\n",
    "        self.linear1 = torch.nn.Linear(d_in, h_in)\n",
    "        self.linear2 = torch.nn.Linear(h_in, d_out)\n",
    "        \n",
    "        self.num_layer = num_layer\n",
    "        self.h_in = h_in\n",
    "        self.hidden = self.init_hidden()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        tem = self.linear1(x)\n",
    "        out, _ = self.lstm(tem.view(len(tem), 1, -1), self.hidden)\n",
    "        y_pred = self.linear2(out.view(len(x), -1))\n",
    "        return y_pred\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(self.num_layer * 1, 1, self.h_in),\n",
    "               torch.zeros(self.num_layer * 1, 1, self.h_in))\n",
    "\n",
    "\n",
    "model = lstm(D_in, D_out, 1, H_in)    \n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=Learning_rate)\n",
    "\n",
    "start = time.time()\n",
    "for t in range(2000):\n",
    "    #scheduler.step\n",
    "    model.hidden = model.init_hidden()\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    # Print and plot every now and then\n",
    "    if t % 100 == 0:\n",
    "        print('Iter-{}; Loss: {:.4}'.format(t, loss.data[0]))\n",
    "end = time.time()\n",
    "print('LSTM cal_time is {}'.format(end-start))\n",
    "\n",
    "y_pred_val = model(x_val)\n",
    "y_pred_val = y_pred_val.reshape(y_pred_val.shape[0], D_out)\n",
    "loss_val = loss_fn(y_pred_val, y_val)\n",
    "print('LSTM val_loss is {}'.format(loss_val.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-0; Loss: 3.581\n",
      "Iter-1000; Loss: 2.054\n",
      "Iter-2000; Loss: 1.371\n",
      "Iter-3000; Loss: 1.052\n",
      "Iter-4000; Loss: 0.8661\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-91a9d3c131f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\M\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\M\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for t in range(18000):\n",
    "    #scheduler.step\n",
    "    model.hidden = model.init_hidden()\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    # Print and plot every now and then\n",
    "    if t % 1000 == 0:\n",
    "        print('Iter-{}; Loss: {:.4}'.format(t, loss.data[0]))\n",
    "\n",
    "y_pred_val = model(x_val)\n",
    "y_pred_val = y_pred_val.reshape(y_pred_val.shape[0], D_out)\n",
    "loss_val = loss_fn(y_pred_val, y_val)\n",
    "print('LSTM val_loss is {}'.format(loss_val.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" VAE模型\n",
    "\n",
    "自定义初始化\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / np.sqrt(in_dim / 2.)\n",
    "    return Variable(torch.randn(*size) * xavier_stddev, requires_grad=True)\n",
    "\n",
    "或调用官方方法\n",
    ">>> w = torch.empty(3, 5)\n",
    ">>> torch.nn.init.xavier_normal_(w)\n",
    "\"\"\"\n",
    "\n",
    "class vae(torch.nn.Module):\n",
    "    def __init__(self, z_dim, x_dim, y_dim, h_dim):\n",
    "        super(vae, self).__init__()\n",
    "        self.Wxh = Variable(torch.nn.init.xavier_normal_(torch.empty(h_dim, x_dim)), requires_grad=True)\n",
    "        self.bxh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "     \n",
    "        self.Whz_mu = Variable(torch.nn.init.xavier_normal_(torch.empty(z_dim, h_dim)), requires_grad=True)  # 需要更改\n",
    "        self.bhz_mu = Variable(torch.zeros(z_dim), requires_grad=True)\n",
    "\n",
    "        self.Whz_var = Variable(torch.nn.init.xavier_normal_(torch.empty(z_dim, h_dim)), requires_grad=True)  # 需要更改\n",
    "        self.bhz_var = Variable(torch.zeros(z_dim), requires_grad=True)\n",
    "\n",
    "        #self.Wzh_tem = Variable(torch.nn.init.xavier_normal_(torch.empty(h_dim, z_dim)), requires_grad=True)\n",
    "        #self.bzh_tem = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "        \n",
    "        self.Wzx = Variable(torch.nn.init.xavier_normal_(torch.empty(y_dim, z_dim)), requires_grad=True)  # 需要更改\n",
    "        self.bzx = Variable(torch.zeros(y_dim), requires_grad=True)\n",
    "        \n",
    "        self.params = [self.Whz_mu, self.bhz_mu, self.Whz_var, self.bhz_var, self.Wzx, self.bzx]\n",
    "        self.params.append(self.Wxh)\n",
    "        self.params.append(self.bxh)\n",
    "        #self.params.append(self.Wzh_tem)\n",
    "        #self.params.append(self.bzh_tem)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # See the autograd section for explanation of what happens here.\n",
    "        self.z_mu, self.z_var = self.Q(x)\n",
    "        z = self.sample_z(self.z_mu, self.z_var)\n",
    "        y_pred = self.P(z)\n",
    "        return y_pred\n",
    "    \n",
    "    # =============================== Q(z|X) ======================================\n",
    "    def Q(self, x):\n",
    "        h = F.relu(F.linear(x, self.Wxh, self.bxh))\n",
    "        z_mu = F.linear(h, self.Whz_mu, self.bhz_mu)\n",
    "        z_var = F.linear(h, self.Whz_var, self.bhz_var)\n",
    "        return z_mu, z_var\n",
    "\n",
    "    def sample_z(self, mu, log_var):\n",
    "        eps = Variable(torch.randn(mu.shape[0], mu.shape[1]))\n",
    "        return mu + torch.exp(log_var / 2) * eps\n",
    "\n",
    "    # =============================== P(X|z) ======================================\n",
    "    def P(self, z):\n",
    "        #h = F.linear(z, self.Wzh_tem, self.bzh_tem)\n",
    "        X = F.linear(z, self.Wzx, self.bzx)\n",
    "        return X\n",
    "    \n",
    "# =============================== TRAINING ====================================  \n",
    "model = vae(z_dim = Z_dim, x_dim = D_in, y_dim = D_out, h_dim = H_in)   \n",
    "optimizer = torch.optim.Adam(model.params, lr=Learning_rate)\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-0; Loss: 51.16\n",
      "Iter-10; Loss: 3.57\n",
      "Iter-20; Loss: 3.811\n",
      "Iter-30; Loss: 1.582\n",
      "Iter-40; Loss: 1.503\n",
      "Iter-50; Loss: 1.655\n",
      "Iter-60; Loss: 1.333\n",
      "Iter-70; Loss: 2.648\n",
      "Iter-80; Loss: 1.337\n"
     ]
    }
   ],
   "source": [
    "# mini-batch\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(1000):   # 训练所有!整套!数据\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):  # 每一步 loader 释放一小批数据用来学习\n",
    "        # Forward\n",
    "        y_pred = model(batch_x)\n",
    "\n",
    "        # Loss\n",
    "        recon_loss = F.mse_loss(y_pred, batch_y, size_average=False)\n",
    "        kl_loss = torch.mean(0.5 * torch.sum(torch.exp(model.z_var) + model.z_mu**2 - 1. - model.z_var, 1))\n",
    "        loss = recon_loss + kl_loss\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print and plot every now and then\n",
    "    if epoch % 10 == 0:\n",
    "        print('Iter-{}; Loss: {:.4}'.format(epoch, recon_loss.data[0]))\n",
    "end = time.time()        \n",
    "print('VAE cal_time is {}'.format(end-start))\n",
    "\n",
    "y_pred_val = model(x_val)\n",
    "recon_loss_val = F.mse_loss(y_pred_val, y_val, size_average=False)\n",
    "print('VAE val_loss is {}'.format(recon_loss_val.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-0; Loss: 3.115\n",
      "Iter-1000; Loss: 2.977\n",
      "Iter-2000; Loss: 3.125\n",
      "Iter-3000; Loss: 2.98\n",
      "Iter-4000; Loss: 2.927\n",
      "Iter-5000; Loss: 2.945\n",
      "Iter-6000; Loss: 2.8\n",
      "Iter-7000; Loss: 2.866\n",
      "Iter-8000; Loss: 2.994\n",
      "Iter-9000; Loss: 2.916\n",
      "Iter-10000; Loss: 2.948\n",
      "Iter-11000; Loss: 3.086\n",
      "Iter-12000; Loss: 2.898\n",
      "Iter-13000; Loss: 2.908\n",
      "Iter-14000; Loss: 2.763\n",
      "Iter-15000; Loss: 2.933\n",
      "Iter-16000; Loss: 2.813\n",
      "Iter-17000; Loss: 2.787\n",
      "Iter-18000; Loss: 2.873\n",
      "Iter-19000; Loss: 2.902\n",
      "VAE net cal_time is 17.38748788833618\n",
      "VAE val_loss is 1.361611247062683\n"
     ]
    }
   ],
   "source": [
    "# Whole batch\n",
    "\n",
    "start = time.time()\n",
    "for t in range(20000):\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    recon_loss = F.mse_loss(y_pred, y, size_average=False)\n",
    "    kl_loss = torch.mean(0.5 * torch.sum(torch.exp(model.z_var) + model.z_mu**2 - 1. - model.z_var, 1))\n",
    "    loss = recon_loss + kl_loss\n",
    "    \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #scheduler.step(loss)\n",
    "    \n",
    "    # Print and plot every now and then\n",
    "    if t % 1000 == 0:\n",
    "        print('Iter-{}; Loss: {:.4}'.format(t, recon_loss.data[0]))\n",
    "end = time.time()\n",
    "print('VAE net cal_time is {}'.format(end-start))\n",
    "y_pred_val = model(x_val)\n",
    "recon_loss_val = F.mse_loss(y_pred_val, y_val, size_average=False)\n",
    "print('VAE val_loss is {}'.format(recon_loss_val.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.params, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.72489643  0.58723855  0.29059172  0.72132415  0.5884636   0.29146591\n",
      "   0.71748704  0.58874875  0.29252183  0.71353656  0.58920979  0.29329067\n",
      "   0.70901805  0.58949703  0.29402959]]\n",
      "[[ 0.72544062  0.58681333  0.29145133  0.72657627  0.58678436  0.29277164\n",
      "   0.72398645  0.5860998   0.29240772  0.72330785  0.58590436  0.29178232\n",
      "   0.72071803  0.5858345   0.29009813]]\n"
     ]
    }
   ],
   "source": [
    "pred_show = y_pred_val[:1].clone().detach().numpy()\n",
    "val_show = y_val[:1].clone().detach().numpy()\n",
    "pred_show.reshape((-1, 3))\n",
    "val_show.reshape((-1, 3))\n",
    "print(pred_show)\n",
    "print(val_show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.params, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_display = y_pred_val.detach().numpy()\n",
    "y_display = y_display.astype('float').reshape(-1, 3)\n",
    "y_display = np.multiply(y_display, (maxdata - mindata)) + mindata\n",
    "#print(y_display)\n",
    "\n",
    "y_display_val = y_val.detach().numpy()\n",
    "y_display_val = y_display_val.astype('float').reshape(-1, 3)\n",
    "y_display_val = np.multiply(y_display_val, (maxdata - mindata)) + mindata\n",
    "#print(y_display_val)\n",
    "\n",
    "#data = pd.DataFrame(y_pred_val.detach().numpy())\n",
    "data = pd.DataFrame(y_display)\n",
    "data.to_csv(\"./model_output/y_display.csv\",index=False,header=False)\n",
    "data = pd.DataFrame(y_display_val)\n",
    "data.to_csv(\"./model_output/y_display_val.csv\",index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 仅保存和加载模型参数(推荐使用)\n",
    "print(model.state_dict())\n",
    "torch.save(model.state_dict(), 'params.pkl')\n",
    "\n",
    "model.load_state_dict(torch.load('params_prone.pkl'))\n",
    "\n",
    "params = model.state_dict()\n",
    "for k,_ in params.items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w1 = params['0.weight'].detach().numpy()\n",
    "print(w1.shape)\n",
    "data = pd.DataFrame(w1)\n",
    "data.to_csv(\"./data/w1.csv\",index=False,header=False)\n",
    "\n",
    "b1 = params['0.bias'].detach().numpy()\n",
    "print(b1.shape)\n",
    "data = pd.DataFrame(b1)\n",
    "data.to_csv(\"./data/b1.csv\",index=False,header=False)\n",
    "\n",
    "w2 = params['2.weight'].detach().numpy()\n",
    "print(w2.shape)\n",
    "data = pd.DataFrame(w2)\n",
    "data.to_csv(\"./data/w2.csv\",index=False,header=False)\n",
    "\n",
    "b2 = params['2.bias'].detach().numpy()\n",
    "print(b2.shape)\n",
    "data = pd.DataFrame(b2)\n",
    "data.to_csv(\"./data/b2.csv\",index=False,header=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
